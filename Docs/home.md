
# Proyecto Onion â€“ Plataforma Abierta de AnÃ¡lisis de InformaciÃ³n

**Onion** es una plataforma modular, automatizada y de cÃ³digo abierto para la recolecciÃ³n, anÃ¡lisis y visualizaciÃ³n de informaciÃ³n relevante sobre Las vulnerabilidades IT y OT (TecnologÃ­as de la InformaciÃ³n y TecnologÃ­as de OperaciÃ³n). Su objetivo es facilitar el acceso a datos estructurados y procesados a partir de fuentes pÃºblicas, con enfoque en la transparencia, la colaboraciÃ³n abierta y el uso de metodologÃ­as de inteligencia.

El proyecto estÃ¡ orientado tanto a investigadores, periodistas de datos y analistas, como a desarrolladores interesados en contribuir con nuevas funcionalidades y dominios de anÃ¡lisis.

---

##  Definiciones



## ğŸ§­ Objetivo del proyecto

Crear una herramienta abierta, extensible y replicable que permita:

- Recolectar noticias y documentos desde mÃºltiples fuentes web.
- Procesar informaciÃ³n no estructurada usando tÃ©cnicas de NLP y machine learning.
- Detectar patrones, narrativas y entidades clave dentro de los textos.
- Correlacionar datos de fuentes heterogÃ©neas (noticias, estadÃ­sticas, vulnerabilidades).
- Visualizar resultados mediante dashboards e interfaces interactivas.
- Servir como base para entrenar modelos de lenguaje adaptados a casos concretos.

---

## ğŸ§© CaracterÃ­sticas principales

- ğŸ” **OrquestaciÃ³n con Apache Airflow**: para automatizar flujos de recolecciÃ³n y anÃ¡lisis.
- ğŸŒ **Multifuente**: integraciones con RSS (TinyRSS), Google Alerts, Google Dorking, bases de datos pÃºblicas, APIs de seguridad, etc.
- ğŸ§  **Procesamiento semÃ¡ntico**: extracciÃ³n de keywords, NER, sentimiento, embeddings vectoriales.
- ğŸ—‚ï¸ **Almacenamiento hÃ­brido**: OpenSearch para bÃºsquedas semÃ¡nticas.
- ğŸ“Š **VisualizaciÃ³n abierta**: dashboards configurables con herramientas como Grafana o Chartbrew.
- ğŸ§± **Arquitectura modular**: diseÃ±ada para incorporar nuevos dominios de anÃ¡lisis de forma independiente.

---

## ğŸ§ª Casos de uso iniciales

union estÃ¡ preparado para adaptarse a distintos dominios temÃ¡ticos. Algunos de los primeros mÃ³dulos en desarrollo incluyen:

- **AnÃ¡lisis de vulnerabilidades tecnolÃ³gicas**: detecciÃ³n y correlaciÃ³n de CVEs, CWE, CAPEC, y noticias sobre ciberseguridad.
- **Entrenamiento de un LLM sobre ciberseguridad industrial**: detecciÃ³n y correlaciÃ³n de CVEs, CWE, CAPEC, y noticias sobre ciberseguridad.


---

## ğŸš€ Objetivo como proyecto open source

- Fomentar la colaboraciÃ³n entre comunidades tÃ©cnicas y acadÃ©micas.
- Proveer una infraestructura reutilizable para proyectos de investigaciÃ³n aplicada.
- Crear un ecosistema de plugins y mÃ³dulos que permita ampliar las capacidades de la plataforma.
- Servir como punto de partida para iniciativas pÃºblicas o ciudadanas de anÃ¡lisis e inteligencia de datos.

---

## ğŸŒ Acceso a la plataforma

HabrÃ¡ una instancia pÃºblica en lÃ­nea accesible desde web para explorar mÃ³dulos activos como vivienda o desinformaciÃ³n. TambiÃ©n se podrÃ¡ clonar e instalar localmente o adaptar para nuevos fines.

---

## Definiciones

### **OSINT (Open Source Intelligence)**  

**DefiniciÃ³n**:  
El **OSINT** (Inteligencia de Fuentes Abiertas) es una metodologÃ­a para recopilar, analizar y aprovechar informaciÃ³n de **fuentes de acceso pÃºblico** con el fin de generar conocimiento Ãºtil. Se centra en datos disponibles legalmente, sin requerir tÃ©cnicas intrusivas o ilegales.

#### **CaracterÃ­sticas clave**:  
- **Fuentes**: Redes sociales, sitios web, foros, registros gubernamentales, artÃ­culos, metadatos, imÃ¡genes, y cualquier recurso pÃºblico.  
- **PropÃ³sito**:  
  - Apoyar investigaciones (periodÃ­sticas, policiales, corporativas).  
  - Identificar riesgos de seguridad (fugas de datos, vulnerabilidades).  
  - Analizar tendencias o comportamientos en redes sociales.  
- **Ã‰tica**: Se basa en el uso responsable de informaciÃ³n pÃºblica, respetando la privacidad y leyes locales.  

#### **Ejemplos de aplicaciones**:  
- Periodistas: Verificar datos para reportajes.  
- Equipos de ciberseguridad: Detectar exposiciones de datos sensibles.  
- Empresas: Estudiar a la competencia o proteger su reputaciÃ³n.  

#### **Herramientas asociadas**:  
- Buscadores avanzados (Google Dorking).  
- Shodan (dispositivos IoT expuestos).  
- Maltego (mapeo de relaciones entre datos).  
- theHarvester (recolecciÃ³n de correos y dominios).  

### **TÃ©cnicas de ExtracciÃ³n de Datos**  

**DefiniciÃ³n**:  
Procesos para obtener, transformar y estructurar informaciÃ³n cruda (texto, imÃ¡genes, metadatos, etc.) desde fuentes heterogÃ©neas (webs, documentos, PDFs) a formatos Ãºtiles (CSV, JSON, bases de datos).  

#### **1. Web Scraping (ExtracciÃ³n de datos de sitios web)**  
- **PropÃ³sito**: Automatizar la recolecciÃ³n de datos estructurados desde pÃ¡ginas web.  
- **Herramientas**:  
  - **Scrapy** (framework en Python para scraping avanzado).  
  - **Beautiful Soup** (biblioteca para parsear HTML/XML).  
- **CÃ³mo funciona**:  
  - Identifica patrones en el HTML (etiquetas, clases CSS).  
  - Extrae texto, enlaces, tablas o imÃ¡genes.  
  - Almacena los datos en formatos como JSON o CSV. 

### **2. Procesamiento de Documentos**  

**PropÃ³sito**: Extraer texto, tablas o metadatos de PDFs, Word, Excel, etc.  

**Herramientas**:  
- **Apache Tika (Java/Python)**: ExtracciÃ³n de contenido y metadatos.  
  ```bash  
  # Usar Tika desde lÃ­nea de comandos  
  java -jar tika-app.jar --text documento.pdf  
- **APyPDF2 (Python)**: ManipulaciÃ³n bÃ¡sica de PDFs.

### **3. ExtracciÃ³n de Metadatos**  

**PropÃ³sito**:Obtener informaciÃ³n oculta (autor, GPS, fecha de creaciÃ³n).
**Herramientas**:  
- **ExifTool**: Metadatos en imÃ¡genes, PDFs y videos.

## **Procesamiento de Texto y CategorizaciÃ³n con NLP/ML**  

### **1. Procesamiento de Texto (NLP)**  

**PropÃ³sito**:  
Transformar texto no estructurado en informaciÃ³n estructurada (keywords, entidades, temas) para clasificar y priorizar datos en herramientas de inteligencia como **MISP**.  

**TÃ©cnicas Clave**:  

#### **A. Preprocesamiento de Texto**  
- **TokenizaciÃ³n**: Dividir texto en palabras o frases.  

#### **B. ExtracciÃ³n de Keywords**  
- **TokenizaciÃ³n**: Dividir texto en palabras o frases. 

##### **B. ExtracciÃ³n de Keywords**  

-TF-IDF (Frecuencia TÃ©rmino-Inverso de Documento): Identifica palabras clave relevantes en un corpus de documentos. 
- RAKE (Rapid Automatic Keyword Extraction): Extrae frases clave basadas en frecuencia y co-ocurrencia.

##### **C. Reconocimiento de Entidades (NER)**  

- Identificar entidades como organizaciones, ubicaciones, o indicadores de compromiso (IOCs).

### **2. Machine Learning para CategorizaciÃ³n

**PropÃ³sito**: Clasificar automÃ¡ticamente la informaciÃ³n en categorÃ­as Ãºtiles para inteligencia (ej: "phishing", "malware", "vulnerabilidades").

**TÃ©cnicas Clave**: 

##### **A. Aprendizaje Supervisado**

**ClasificaciÃ³n de Texto**: Con modelos como SVM, Random Forest, Redes Neuronales (LSTM, Transformers). Se debe Etiquetar datos manualmente y Entrenar un modelo con embeddings.

##### **B. Aprendizaje No Supervisado**

- *Clustering*: Agrupar documentos similares sin etiquetas previas.
- *Topic Modeling*: Descubrir temas ocultos en documentos (ej: LDA).

> ğŸ’¡ *Creemos que la inteligencia de datos debe estar al servicio de la sociedad. Este proyecto es nuestra apuesta por una tecnologÃ­a transparente, abierta y colaborativa.*
